{
  "status": "success",
  "data": {
    "groups": [
      {
        "name": "Docker alerts",
        "file": "/prometheus_config/alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Service has scaled",
            "query": "round(delta(my_container_replicas_by_service[1m])) != 0",
            "duration": 1,
            "labels": {
              "info": "Service {{$labels.container_label_com_docker_swarm_service_name}} has scaled",
              "priority": "medium",
              "severity": "informational",
              "tags": "docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "Service {{$labels.container_label_com_docker_swarm_service_name}} has just scaled instances for {{ humanize $value}}",
              "summary": "Service has scaled"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000560785,
            "lastEvaluation": "2021-12-21T15:41:34.409084419Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "High Cpu Usage On Container",
            "query": "sum by(container_label_com_docker_swarm_task_name, instance) (rate(container_cpu_usage_seconds_total{container_label_com_docker_swarm_task_name=~\".+\"}[5m])) * 100 > 200",
            "duration": 300,
            "labels": {
              "alarmcode": "7600010",
              "info": "High CPU usage: TASK '{{ $labels.container_label_com_docker_swarm_task_name }}' on '{{ $labels.instance }}'",
              "instance": "{{$labels.instance}}",
              "severity": "warning",
              "tags": "docker, swarm, container, cpu"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "{{ $labels.container_label_com_docker_swarm_task_name }} is using a LOT of CPU. CPU usage is {{ humanize $value}}%.",
              "summary": "High CPU usage: TASK '{{ $labels.container_label_com_docker_swarm_task_name }}' on '{{ $labels.instance }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002963294,
            "lastEvaluation": "2021-12-21T15:41:34.409646733Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Container Eating Memory",
            "query": "sum by(instance, name) (container_memory_rss{container_label_com_docker_swarm_task_name=~\".+\"}) > 2.5e+09",
            "duration": 300,
            "labels": {
              "alarmcode": "7600020",
              "info": "High memory usage: TASK '{{ $labels.name }}' on '{{ $labels.instance }}'",
              "severity": "warning",
              "tags": "docker, swarm, container, memory"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}b",
              "description": "{{ $labels.name }} is eating up a LOT of memory. Memory consumption is at {{ humanize $value}}b.",
              "summary": "High memory usage: TASK '{{ $labels.name }}' on '{{ $labels.instance }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000825191,
            "lastEvaluation": "2021-12-21T15:41:34.412611515Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Container Task Not Running",
            "query": "sum by(container_label_com_docker_stack_namespace, instance) (container_tasks_state{state!=\"running\"}) > 0",
            "duration": 300,
            "labels": {
              "info": "Container not in running state in stack '{{ $labels.container_label_com_docker_swarm_task_name }}' on '{{ $labels.instance }}'",
              "severity": "informational",
              "tags": "docker, swarm, container, stopped"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "{{ $labels.container_label_com_docker_swarm_task_name }} has a container not in running state on {{ $labels.instance }}",
              "summary": "Container not in running state in stack '{{ $labels.container_label_com_docker_swarm_task_name }}' on '{{ $labels.instance }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.005546343,
            "lastEvaluation": "2021-12-21T15:41:34.413438415Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Missing Node Exporter",
            "query": "count(container_last_seen{container_label_com_docker_stack_namespace=\"prom\",container_label_com_docker_swarm_service_name=~\".*node.*\"}) < count(swarm_node_info)",
            "duration": 60,
            "labels": {
              "alarmcode": "8400090",
              "info": "A container node exporter is missing",
              "instance": "{{$labels.instance}}",
              "severity": "warning",
              "tags": "docker, swarm"
            },
            "annotations": {
              "description": "A node-expoter container is not running on a host {{$labels.instance}}",
              "summary": "A container node exporter is missing"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "8400090",
                  "alertname": "Missing Node Exporter",
                  "info": "A container node exporter is missing",
                  "severity": "warning",
                  "tags": "docker, swarm"
                },
                "annotations": {
                  "description": "A node-expoter container is not running on a host ",
                  "summary": "A container node exporter is missing"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:19.407369467Z",
                "value": "2e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000975648,
            "lastEvaluation": "2021-12-21T15:41:34.418985805Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.010889701,
        "lastEvaluation": "2021-12-21T15:41:34.4090785Z"
      },
      {
        "name": "Exporter alerts",
        "file": "/prometheus_config/alert_rules.yml",
        "rules": [
          {
            "state": "firing",
            "name": "Exporter Down",
            "query": "up == 0",
            "duration": 300,
            "labels": {
              "alarmcode": "4100130",
              "info": "{{$labels.job}} not responding",
              "label1": "aaa",
              "label2": "bbbbb",
              "priority": "low",
              "severity": "warning",
              "tags": "exporter"
            },
            "annotations": {
              "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
              "description": "Exporter on {{ $labels.instance }} is not reachable.",
              "summary": "Exporter {{$labels.job}} is down!"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "nexus11.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on nexus11.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "openbaton not responding",
                  "instance": "172.29.100.5:8080",
                  "job": "openbaton",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 172.29.100.5:8080 is not reachable.",
                  "summary": "Exporter openbaton is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "coredns not responding",
                  "instance": "coredns:9153",
                  "job": "coredns",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on coredns:9153 is not reachable.",
                  "summary": "Exporter coredns is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "pmon not responding",
                  "instance": "192.168.1.99:8084",
                  "job": "pmon",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 192.168.1.99:8084 is not reachable.",
                  "summary": "Exporter pmon is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "ansibleic.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on ansibleic.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "ping-probe not responding",
                  "instance": "matjazcerkvenik.si",
                  "job": "ping-probe",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on matjazcerkvenik.si is not reachable.",
                  "summary": "Exporter ping-probe is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "rabbitmq not responding",
                  "instance": "rabbitmq-exporter:9419",
                  "job": "rabbitmq",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on rabbitmq-exporter:9419 is not reachable.",
                  "summary": "Exporter rabbitmq is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "ping-exporter not responding",
                  "instance": "ping-exporter:9346",
                  "job": "ping-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on ping-exporter:9346 is not reachable.",
                  "summary": "Exporter ping-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "snmp-exporter not responding",
                  "instance": "172.18.216.215",
                  "job": "snmp-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 172.18.216.215 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "dockerbaton not responding",
                  "instance": "172.29.6.6:18080",
                  "job": "dockerbaton",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 172.29.6.6:18080 is not reachable.",
                  "summary": "Exporter dockerbaton is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "cadvisor not responding",
                  "instance": "mcrk-docker-2:9080",
                  "job": "cadvisor",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on mcrk-docker-2:9080 is not reachable.",
                  "summary": "Exporter cadvisor is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "it-gitlab not responding",
                  "instance": "gitlab.iskratel.si:443",
                  "job": "it-gitlab",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on gitlab.iskratel.si:443 is not reachable.",
                  "summary": "Exporter it-gitlab is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "cadvisor not responding",
                  "instance": "pmon.compact.si:9080",
                  "job": "cadvisor",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on pmon.compact.si:9080 is not reachable.",
                  "summary": "Exporter cadvisor is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "pmonmak.servis.mak:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on pmonmak.servis.mak:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "snmp-exporter not responding",
                  "instance": "192.168.1.222",
                  "job": "snmp-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 192.168.1.222 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "vmgitent.iskratel.si:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on vmgitent.iskratel.si:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "openbaton not responding",
                  "instance": "172.29.100.100:8080",
                  "job": "openbaton",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 172.29.100.100:8080 is not reachable.",
                  "summary": "Exporter openbaton is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "ping-probe not responding",
                  "instance": "172.29.6.6",
                  "job": "ping-probe",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 172.29.6.6 is not reachable.",
                  "summary": "Exporter ping-probe is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "hss.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on hss.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "metricslib not responding",
                  "instance": "mcrk-docker-2:9099",
                  "job": "metricslib",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on mcrk-docker-2:9099 is not reachable.",
                  "summary": "Exporter metricslib is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "cadvisor not responding",
                  "instance": "mcrk-docker-3:9080",
                  "job": "cadvisor",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on mcrk-docker-3:9080 is not reachable.",
                  "summary": "Exporter cadvisor is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "mns.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on mns.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "ping-probe not responding",
                  "instance": "centosvm",
                  "job": "ping-probe",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on centosvm is not reachable.",
                  "summary": "Exporter ping-probe is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "snmp-exporter not responding",
                  "instance": "172.18.216.216",
                  "job": "snmp-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 172.18.216.216 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "dns-enum-slave.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on dns-enum-slave.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "sipcscf.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on sipcscf.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "traefik not responding",
                  "instance": "mcrk-docker-1:8085",
                  "job": "traefik",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on mcrk-docker-1:8085 is not reachable.",
                  "summary": "Exporter traefik is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "snmp-exporter not responding",
                  "instance": "172.18.212.76",
                  "job": "snmp-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 172.18.212.76 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "ping-probe not responding",
                  "instance": "vmgitent.iskratel.si",
                  "job": "ping-probe",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on vmgitent.iskratel.si is not reachable.",
                  "summary": "Exporter ping-probe is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "cadvisor not responding",
                  "instance": "mcrk-docker-1:9080",
                  "job": "cadvisor",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on mcrk-docker-1:9080 is not reachable.",
                  "summary": "Exporter cadvisor is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "snmp-exporter not responding",
                  "instance": "192.168.133.3",
                  "job": "snmp-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on 192.168.133.3 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:07.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "nexus.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on nexus.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "ping-probe not responding",
                  "instance": "prometheus.io",
                  "job": "ping-probe",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on prometheus.io is not reachable.",
                  "summary": "Exporter ping-probe is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "ping-probe not responding",
                  "instance": "gitlab.iskratel.si",
                  "job": "ping-probe",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on gitlab.iskratel.si is not reachable.",
                  "summary": "Exporter ping-probe is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100130",
                  "alertname": "Exporter Down",
                  "info": "node-exporter not responding",
                  "instance": "dns-enum-master.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "label1": "aaa",
                  "label2": "bbbbb",
                  "priority": "low",
                  "severity": "warning",
                  "tags": "exporter"
                },
                "annotations": {
                  "console": "Check the Grafana Dashboard at http://${GRAFANA_HOSTNAME}:3000",
                  "description": "Exporter on dns-enum-master.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.014851086,
            "lastEvaluation": "2021-12-21T15:41:37.79656006Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Server Is Not Monitored",
            "query": "up{job=~\"node-exporter.*\"} == 0",
            "duration": 600,
            "labels": {
              "alarmcode": "4100120",
              "info": "{{$labels.job}} not responding",
              "priority": "low",
              "severity": "major",
              "tags": "exporter, node, server"
            },
            "annotations": {
              "description": "Exporter on {{ $labels.instance }} is not reachable.",
              "summary": "Exporter {{$labels.job}} is down"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "dns-enum-slave.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on dns-enum-slave.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-11-03T16:12:37Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "ansibleic.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on ansibleic.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-11-29T14:34:37Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "nexus11.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on nexus11.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-11-18T21:39:37Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "pmonmak.servis.mak:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on pmonmak.servis.mak:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-10-21T13:12:22Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "vmgitent.iskratel.si:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on vmgitent.iskratel.si:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-10-21T13:12:22Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "dns-enum-master.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on dns-enum-master.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-10-21T13:12:22Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "hss.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on hss.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-12-13T14:38:37Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "mns.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on mns.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-11-29T14:34:37Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "nexus.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on nexus.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-11-18T21:39:22Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "4100120",
                  "alertname": "Server Is Not Monitored",
                  "info": "node-exporter not responding",
                  "instance": "sipcscf.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "major",
                  "tags": "exporter, node, server"
                },
                "annotations": {
                  "description": "Exporter on sipcscf.devops.iskratel.cloud:9100 is not reachable.",
                  "summary": "Exporter node-exporter is down"
                },
                "state": "firing",
                "activeAt": "2021-11-03T16:12:37Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.003384313,
            "lastEvaluation": "2021-12-21T15:41:37.811416771Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "SNMP Is Not Monitored",
            "query": "up{job=\"snmp-exporter\"} == 0",
            "duration": 600,
            "labels": {
              "alarmcode": "2809010",
              "info": "Exporter {{$labels.job}} not responding!",
              "priority": "medium",
              "severity": "major",
              "tags": "exporter, snmp"
            },
            "annotations": {
              "description": "Exporter on {{ $labels.instance }} is not reachable.",
              "summary": "Exporter {{$labels.job}} is down!"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "2809010",
                  "alertname": "SNMP Is Not Monitored",
                  "info": "Exporter snmp-exporter not responding!",
                  "instance": "192.168.1.222",
                  "job": "snmp-exporter",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "exporter, snmp"
                },
                "annotations": {
                  "description": "Exporter on 192.168.1.222 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "2809010",
                  "alertname": "SNMP Is Not Monitored",
                  "info": "Exporter snmp-exporter not responding!",
                  "instance": "192.168.133.3",
                  "job": "snmp-exporter",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "exporter, snmp"
                },
                "annotations": {
                  "description": "Exporter on 192.168.133.3 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:07.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "2809010",
                  "alertname": "SNMP Is Not Monitored",
                  "info": "Exporter snmp-exporter not responding!",
                  "instance": "172.18.212.76",
                  "job": "snmp-exporter",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "exporter, snmp"
                },
                "annotations": {
                  "description": "Exporter on 172.18.212.76 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "2809010",
                  "alertname": "SNMP Is Not Monitored",
                  "info": "Exporter snmp-exporter not responding!",
                  "instance": "172.18.216.215",
                  "job": "snmp-exporter",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "exporter, snmp"
                },
                "annotations": {
                  "description": "Exporter on 172.18.216.215 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "2809010",
                  "alertname": "SNMP Is Not Monitored",
                  "info": "Exporter snmp-exporter not responding!",
                  "instance": "172.18.216.216",
                  "job": "snmp-exporter",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "exporter, snmp"
                },
                "annotations": {
                  "description": "Exporter on 172.18.216.216 is not reachable.",
                  "summary": "Exporter snmp-exporter is down!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001541321,
            "lastEvaluation": "2021-12-21T15:41:37.814803666Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Application Is Not Monitored",
            "query": "up{job=~\"dockerbaton|openbaton|pmon|rabbitmq\"} == 0",
            "duration": 600,
            "labels": {
              "alarmcode": "100170",
              "info": "Exporter {{$labels.job}} is down!",
              "priority": "low",
              "severity": "minor",
              "tags": "exporter, application"
            },
            "annotations": {
              "description": "Exporter on {{ $labels.instance }} is not reachable.",
              "summary": "Exporter {{$labels.job}} not responding!"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "100170",
                  "alertname": "Application Is Not Monitored",
                  "info": "Exporter openbaton is down!",
                  "instance": "172.29.100.5:8080",
                  "job": "openbaton",
                  "priority": "low",
                  "severity": "minor",
                  "tags": "exporter, application"
                },
                "annotations": {
                  "description": "Exporter on 172.29.100.5:8080 is not reachable.",
                  "summary": "Exporter openbaton not responding!"
                },
                "state": "firing",
                "activeAt": "2021-12-07T16:16:52Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "100170",
                  "alertname": "Application Is Not Monitored",
                  "info": "Exporter dockerbaton is down!",
                  "instance": "172.29.6.6:18080",
                  "job": "dockerbaton",
                  "priority": "low",
                  "severity": "minor",
                  "tags": "exporter, application"
                },
                "annotations": {
                  "description": "Exporter on 172.29.6.6:18080 is not reachable.",
                  "summary": "Exporter dockerbaton not responding!"
                },
                "state": "firing",
                "activeAt": "2021-12-07T16:16:52Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "100170",
                  "alertname": "Application Is Not Monitored",
                  "info": "Exporter rabbitmq is down!",
                  "instance": "rabbitmq-exporter:9419",
                  "job": "rabbitmq",
                  "priority": "low",
                  "severity": "minor",
                  "tags": "exporter, application"
                },
                "annotations": {
                  "description": "Exporter on rabbitmq-exporter:9419 is not reachable.",
                  "summary": "Exporter rabbitmq not responding!"
                },
                "state": "firing",
                "activeAt": "2021-10-21T13:12:22Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "100170",
                  "alertname": "Application Is Not Monitored",
                  "info": "Exporter pmon is down!",
                  "instance": "192.168.1.99:8084",
                  "job": "pmon",
                  "priority": "low",
                  "severity": "minor",
                  "tags": "exporter, application"
                },
                "annotations": {
                  "description": "Exporter on 192.168.1.99:8084 is not reachable.",
                  "summary": "Exporter pmon not responding!"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:52.795336553Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alarmcode": "100170",
                  "alertname": "Application Is Not Monitored",
                  "info": "Exporter openbaton is down!",
                  "instance": "172.29.100.100:8080",
                  "job": "openbaton",
                  "priority": "low",
                  "severity": "minor",
                  "tags": "exporter, application"
                },
                "annotations": {
                  "description": "Exporter on 172.29.100.100:8080 is not reachable.",
                  "summary": "Exporter openbaton not responding!"
                },
                "state": "firing",
                "activeAt": "2021-12-13T14:38:37Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001635673,
            "lastEvaluation": "2021-12-21T15:41:37.816347051Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Openbaton not all replicas running",
            "query": "avg_over_time(openbaton_docker_number_of_running_replikas[5m]) - avg_over_time(openbaton_docker_number_of_requested_replicas[5m]) != 0",
            "duration": 120,
            "labels": {
              "alarmcode": "100170",
              "info": "Number of running replicas is different than number of requested replicas on {{$labels.instance}}.",
              "priority": "low",
              "severity": "minor",
              "tags": "openbaton, application"
            },
            "annotations": {
              "description": "Number of running replicas is different than number of requested replicas on {{$labels.instance}}.",
              "summary": "Openbaton not all replicas running on {{$labels.instance}}."
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "100170",
                  "alertname": "Openbaton not all replicas running",
                  "info": "Number of running replicas is different than number of requested replicas on 10.0.35.40:9100.",
                  "instance": "10.0.35.40:9100",
                  "job": "node-exporter",
                  "priority": "low",
                  "severity": "minor",
                  "tags": "openbaton, application"
                },
                "annotations": {
                  "description": "Number of running replicas is different than number of requested replicas on 10.0.35.40:9100.",
                  "summary": "Openbaton not all replicas running on 10.0.35.40:9100."
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:22.795336553Z",
                "value": "-8e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000575257,
            "lastEvaluation": "2021-12-21T15:41:37.817984747Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.0220096,
        "lastEvaluation": "2021-12-21T15:41:37.796552989Z"
      },
      {
        "name": "IT devops alerts",
        "file": "/prometheus_config/alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Gitlab HTTP not responding",
            "query": "probe_success{instance=\"https://gitlab.iskratel.si\",job=\"blackbox-http\"} == 0",
            "duration": 120,
            "labels": {
              "alarmcode": "300070",
              "info": "Gitlab is not responding to HTTP requests",
              "priority": "high",
              "severity": "major",
              "tags": "http, gitlab, devops",
              "url": "https://${PROM_DEVOPS_SERVER}/alertmonitor/"
            },
            "annotations": {
              "description": "Gitlab on {{$labels.instance}} has been down for more than 2 minutes.",
              "summary": "Gitlab HTTP not responding"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000403086,
            "lastEvaluation": "2021-12-21T15:41:35.119066308Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Gitlab server down",
            "query": "probe_success{instance=\"gitlab.iskratel.si\",job=\"blackbox-icmp\"} == 0",
            "duration": 60,
            "labels": {
              "alarmcode": "2809020",
              "info": "ICMP ping failed",
              "priority": "high",
              "severity": "major",
              "tags": "icmp, gitlab, devops",
              "url": "https://${PROM_DEVOPS_SERVER}/alertmonitor/"
            },
            "annotations": {
              "description": "ICMP ping on {{$labels.instance}} has been failing for more than 1 minute.",
              "summary": "Gitlab server down"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000132816,
            "lastEvaluation": "2021-12-21T15:41:35.119470772Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Gitlab SSH Not Responding",
            "query": "probe_success{instance=\"gitlab.iskratel.si\",job=\"blackbox-ssh\"} == 0",
            "duration": 120,
            "labels": {
              "alarmcode": "300070",
              "info": "SSH on {{$labels.instance}} has been down for more than 2 minutes.",
              "severity": "minor",
              "tags": "ssh, gitlab, devops",
              "url": "https://${PROM_DEVOPS_SERVER}/alertmonitor/"
            },
            "annotations": {
              "summary": "SSH on {{$labels.instance}} has been down for more than 2 minutes."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000103555,
            "lastEvaluation": "2021-12-21T15:41:35.119604524Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Certificate expiring soon",
            "query": "(probe_ssl_earliest_cert_expiry{instance=\"https://gitlab.iskratel.si\",job=\"blackbox-http\"} - time()) / 3600 / 24 < 30",
            "duration": 600,
            "labels": {
              "alarmcode": "4100030",
              "info": "SSL cert on '{{$labels.instance}}' will expire soon",
              "severity": "warning",
              "tags": "ssl, gitlab, devops",
              "url": "https://${PROM_DEVOPS_SERVER}/alertmonitor/"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }} days",
              "description": "SSL cert will expire in {{ humanize $value}} days",
              "summary": "SSL cert on '{{$labels.instance}}' will expire soon"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "4100030",
                  "alertname": "Certificate expiring soon",
                  "info": "SSL cert on 'https://gitlab.iskratel.si' will expire soon",
                  "instance": "https://gitlab.iskratel.si",
                  "job": "blackbox-http",
                  "severity": "warning",
                  "tags": "ssl, gitlab, devops",
                  "url": "https://${PROM_DEVOPS_SERVER}/alertmonitor/"
                },
                "annotations": {
                  "currentValue": "-570.2 days",
                  "description": "SSL cert will expire in -570.2 days",
                  "summary": "SSL cert on 'https://gitlab.iskratel.si' will expire soon"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:05.118260905Z",
                "value": "-5.702034388657411e+02"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.00068239,
            "lastEvaluation": "2021-12-21T15:41:35.119708687Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Nexus HTTP not responding",
            "query": "probe_success{instance=\"https://nexus.devops.iskratel.cloud\",job=\"blackbox-http\"} == 0",
            "duration": 60,
            "labels": {
              "alarmcode": "300070",
              "info": "Nexus is not responding to HTTP requests",
              "priority": "high",
              "severity": "critical",
              "tags": "http, nexus, devops",
              "url": "https://${PROM_DEVOPS_SERVER}/alertmonitor/"
            },
            "annotations": {
              "description": "Nexus on {{$labels.instance}} has been down for more than 1 minute.",
              "summary": "Nexus HTTP not responding"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000157025,
            "lastEvaluation": "2021-12-21T15:41:35.120392704Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Nexus server down",
            "query": "probe_success{instance=\"nexus.devops.iskratel.cloud\",job=\"blackbox-icmp\"} == 0",
            "duration": 60,
            "labels": {
              "alarmcode": "2809020",
              "info": "ICMP ping failed",
              "priority": "high",
              "severity": "critical",
              "tags": "icmp, nexus, devops",
              "url": "https://${PROM_DEVOPS_SERVER}/alertmonitor/"
            },
            "annotations": {
              "description": "ICMP ping on {{$labels.instance}} has been failing for more than 1 minute.",
              "summary": "Nexus server down"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00011462,
            "lastEvaluation": "2021-12-21T15:41:35.120550413Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.001608125,
        "lastEvaluation": "2021-12-21T15:41:35.119059273Z"
      },
      {
        "name": "Swarm alerts",
        "file": "/prometheus_config/alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "CPU usage",
            "query": "100 - (avg by(node_name) (irate(node_cpu_seconds_total{mode=\"idle\"}[1m]) * on(instance) group_left(node_name) node_meta * 100)) > 80",
            "duration": 60,
            "labels": {
              "alarmcode": "7600010",
              "info": "CPU alert for Node '{{ $labels.node_name }}'",
              "instance": "{{$labels.node_name}}",
              "nodename": "{{$labels.node_name}}",
              "priority": "high",
              "severity": "critical",
              "tags": "server, cpu, docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "Node {{ $labels.node_name }} CPU usage is at {{ humanize $value}}%.",
              "summary": "CPU alert for Node '{{ $labels.node_name }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002690308,
            "lastEvaluation": "2021-12-21T15:41:34.779835525Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Memory usage",
            "query": "sum by(node_name) (((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * on(instance) group_left(node_name) node_meta * 100) > 80",
            "duration": 60,
            "labels": {
              "alarmcode": "7600020",
              "info": "Memory alert for node '{{ $labels.node_name }}'",
              "instance": "{{$labels.node_name}}",
              "nodename": "{{$labels.node_name}}",
              "priority": "medium",
              "severity": "major",
              "tags": "server, memory, docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "Memory usage is at {{ humanize $value}}%.",
              "summary": "Memory alert for node '{{ $labels.node_name }}'"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "7600020",
                  "alertname": "Memory usage",
                  "info": "Memory alert for node 'openbaton.devops.iskratel.cloud'",
                  "instance": "openbaton.devops.iskratel.cloud",
                  "node_name": "openbaton.devops.iskratel.cloud",
                  "nodename": "openbaton.devops.iskratel.cloud",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "server, memory, docker, swarm"
                },
                "annotations": {
                  "currentValue": "80.86%",
                  "description": "Memory usage is at 80.86%.",
                  "summary": "Memory alert for node 'openbaton.devops.iskratel.cloud'"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:19.778716768Z",
                "value": "8.086032804701597e+01"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001974142,
            "lastEvaluation": "2021-12-21T15:41:34.782528149Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Disk usage",
            "query": "((node_filesystem_size_bytes{fstype=\"rootfs\",mountpoint=\"/\"} - node_filesystem_free_bytes{fstype=\"rootfs\",mountpoint=\"/\"}) * 100 / node_filesystem_size_bytes{fstype=\"rootfs\",mountpoint=\"/\"}) * on(instance) group_left(node_name) node_meta > 85",
            "duration": 1800,
            "labels": {
              "alarmcode": "2801100",
              "info": "Disk alert for Node '{{ $labels.node_name }}'",
              "instance": "{{$labels.node_name}}",
              "nodename": "{{$labels.node_name}}",
              "priority": "medium",
              "severity": "warning",
              "tags": "server, disk space, docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "Node {{ $labels.node_name }} disk usage is at {{ humanize $value}}%.",
              "summary": "Disk alert for Node '{{ $labels.node_name }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001370831,
            "lastEvaluation": "2021-12-21T15:41:34.784505286Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Service missing",
            "query": "count(count by(container_label_com_docker_swarm_service_name) (container_tasks_state{container_label_com_docker_swarm_service_name=~\".+\"} * on(container_label_com_docker_swarm_node_id) group_left(node_name) min without(instance) (node_meta{node_name=~\"$node_name\"}))) - count(rate(container_last_seen[5m]) * on(container_label_com_docker_swarm_node_id) group_left(node_name) min without(instance) (node_meta{node_name=~\"$node_name\"})) > 0",
            "duration": 300,
            "labels": {
              "alarmcode": "3200650",
              "info": "At least 1 container is not running. Should be equal to number of services or more.",
              "instance": "{{$labels.node_name}}",
              "nodename": "{{$labels.node_name}}",
              "priority": "low",
              "severity": "critical",
              "tags": "docker, swarm, container"
            },
            "annotations": {
              "currentValue": "{{ $value }}",
              "description": "At least 1 container is not running.",
              "summary": "Service missing"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.003713854,
            "lastEvaluation": "2021-12-21T15:41:34.78587752Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Var partition almost full",
            "query": "((node_filesystem_size_bytes{mountpoint=\"/rootfs/var\"} - node_filesystem_free_bytes{mountpoint=\"/rootfs/var\"}) * 100 / node_filesystem_size_bytes{mountpoint=\"/rootfs/var\"}) * on(instance) group_left(node_name) node_meta > 80",
            "duration": 1800,
            "labels": {
              "info": "/rootfs/var partition almost full on {{ $labels.node_name}}",
              "instance": "{{$labels.node_name}}",
              "nodename": "{{$labels.node_name}}",
              "priority": "high",
              "severity": "major",
              "tags": "server, disk space, docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "var partition used: {{ humanize $value }}%.",
              "summary": "/rootfs/var partition almost full on {{ $labels.node_name}}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000379787,
            "lastEvaluation": "2021-12-21T15:41:34.789592444Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Disk fill rate (6h)",
            "query": "predict_linear(node_filesystem_free_bytes{fstype=\"rootfs\",mountpoint=\"/\"}[1h], 6 * 3600) * on(instance) group_left(node_name) node_meta < 0",
            "duration": 3600,
            "labels": {
              "info": "Disk fill alert for Node '{{ $labels.node_name }}'",
              "instance": "{{$labels.instance}}",
              "nodename": "{{$labels.node_name}}",
              "priority": "low",
              "severity": "minor",
              "tags": "server, disk space, prediction, docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "Node {{ $labels.node_name }} disk is going to fill up in 6h.",
              "summary": "Disk fill alert for Node '{{ $labels.node_name }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001906178,
            "lastEvaluation": "2021-12-21T15:41:34.789973198Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.012060519,
        "lastEvaluation": "2021-12-21T15:41:34.779823757Z"
      },
      {
        "name": "Weather alerts",
        "file": "/prometheus_config/alert_rules.yml",
        "rules": [
          {
            "state": "firing",
            "name": "Freezing",
            "query": "weather_temperature_celsius < 0",
            "duration": 900,
            "labels": {
              "info": "Temperature is below zero in '{{ $labels.location }}'",
              "severity": "major",
              "tags": "weather, temperature"
            },
            "annotations": {
              "currentValue": "{{ $value }}C",
              "description": "It's freezing in '{{ $labels.location }}'. Current temperature is {{ $value}}C.",
              "summary": "Freezing alert"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'SKOCJAN'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "SKOCJAN",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-0.2C",
                  "description": "It's freezing in 'SKOCJAN'. Current temperature is -0.2C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:52:07.915576732Z",
                "value": "-2e-01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'POSTOJNA'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "POSTOJNA",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-1C",
                  "description": "It's freezing in 'POSTOJNA'. Current temperature is -1C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T15:24:07.915576732Z",
                "value": "-1e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'CERKNISKO JEZERO'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "CERKNISKO JEZERO",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-1.3C",
                  "description": "It's freezing in 'CERKNISKO JEZERO'. Current temperature is -1.3C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-1.3e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'LISCA'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "LISCA",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-4C",
                  "description": "It's freezing in 'LISCA'. Current temperature is -4C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-4e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'RATECE (AMS)'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "RATECE (AMS)",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-4.3C",
                  "description": "It's freezing in 'RATECE (AMS)'. Current temperature is -4.3C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-4.3e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'RATECE'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "RATECE",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-4C",
                  "description": "It's freezing in 'RATECE'. Current temperature is -4C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-4e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'VRHNIKA (AMS)'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "VRHNIKA (AMS)",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-0.9C",
                  "description": "It's freezing in 'VRHNIKA (AMS)'. Current temperature is -0.9C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-9e-01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'VOGEL'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "VOGEL",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-5C",
                  "description": "It's freezing in 'VOGEL'. Current temperature is -5C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-5e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'LOGATEC (AMS)'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "LOGATEC (AMS)",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-0.8C",
                  "description": "It's freezing in 'LOGATEC (AMS)'. Current temperature is -0.8C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-8e-01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'POSTOJNA (AMS)'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "POSTOJNA (AMS)",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-0.7C",
                  "description": "It's freezing in 'POSTOJNA (AMS)'. Current temperature is -0.7C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-7e-01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'BOVEC (AMS)'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "BOVEC (AMS)",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-0.6C",
                  "description": "It's freezing in 'BOVEC (AMS)'. Current temperature is -0.6C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-6e-01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'KAMNISKA BISTRICA'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "KAMNISKA BISTRICA",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-0.9C",
                  "description": "It's freezing in 'KAMNISKA BISTRICA'. Current temperature is -0.9C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:32:07.915576732Z",
                "value": "-9e-01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'SLOVENJ GRADEC'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "SLOVENJ GRADEC",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-2C",
                  "description": "It's freezing in 'SLOVENJ GRADEC'. Current temperature is -2C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T15:25:07.915576732Z",
                "value": "-2e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'RAVNE NA KOROSKEM (AMS)'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "RAVNE NA KOROSKEM (AMS)",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-3C",
                  "description": "It's freezing in 'RAVNE NA KOROSKEM (AMS)'. Current temperature is -3C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-3e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'STOCKHOLM'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "STOCKHOLM",
                  "region": "eu",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-5C",
                  "description": "It's freezing in 'STOCKHOLM'. Current temperature is -5C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-5e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'BABNO POLJE'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "BABNO POLJE",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-1.8C",
                  "description": "It's freezing in 'BABNO POLJE'. Current temperature is -1.8C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-1.8e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'KREDARICA'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "KREDARICA",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-10C",
                  "description": "It's freezing in 'KREDARICA'. Current temperature is -10C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-1e+01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'MOSKVA'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "MOSKVA",
                  "region": "eu",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-16C",
                  "description": "It's freezing in 'MOSKVA'. Current temperature is -16C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:22.915576732Z",
                "value": "-1.6e+01"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'KRANJ (AMS)'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "KRANJ (AMS)",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-1C",
                  "description": "It's freezing in 'KRANJ (AMS)'. Current temperature is -1C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T15:08:07.915576732Z",
                "value": "-1e+00"
              },
              {
                "labels": {
                  "alertname": "Freezing",
                  "info": "Temperature is below zero in 'LESCE'",
                  "instance": "weather-exporter:9885",
                  "job": "weather-exporter",
                  "location": "LESCE",
                  "region": "slo",
                  "severity": "major",
                  "tags": "weather, temperature"
                },
                "annotations": {
                  "currentValue": "-1C",
                  "description": "It's freezing in 'LESCE'. Current temperature is -1C.",
                  "summary": "Freezing alert"
                },
                "state": "firing",
                "activeAt": "2021-12-21T15:25:07.915576732Z",
                "value": "-1e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.005597415,
            "lastEvaluation": "2021-12-21T15:41:37.916450646Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Hot outside",
            "query": "weather_temperature_celsius > 30",
            "duration": 900,
            "labels": {
              "info": "Temperature is more than 30C in '{{ $labels.location }}'",
              "severity": "major",
              "tags": "weather, temperature"
            },
            "annotations": {
              "currentValue": "{{ $value }}C",
              "description": "Current temperature is {{ $value}}C. Stay hidrated!",
              "summary": "Hot temperature in '{{ $labels.location }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000370772,
            "lastEvaluation": "2021-12-21T15:41:37.922051003Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.005981721,
        "lastEvaluation": "2021-12-21T15:41:37.916443207Z"
      },
      {
        "name": "cAdvisor alerts",
        "file": "/prometheus_config/alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Container high CPU usage",
            "query": "irate(container_cpu_usage_seconds_total{name!=\"\"}[5m]) * 100 > 90",
            "duration": 120,
            "labels": {
              "alarmcode": "7600010",
              "info": "Container '{{ $labels.name }}' on {{ $labels.instance }} is using more than 90% of CPU",
              "severity": "major",
              "tags": "docker"
            },
            "annotations": {
              "description": "Container '{{ $labels.name }}' on {{ $labels.instance }} is using more than 90% of CPU",
              "summary": "Container high CPU usage '{{ $labels.name }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002902017,
            "lastEvaluation": "2021-12-21T15:41:46.855428971Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Container high read rate",
            "query": "sum by(instance, name) (irate(container_fs_reads_total{name!=\"\"}[5m])) > 100",
            "duration": 120,
            "labels": {
              "info": "Container '{{ $labels.name }}' on {{ $labels.instance }} has high read rate (> 100 IOPS)",
              "severity": "major",
              "tags": "docker"
            },
            "annotations": {
              "description": "Container '{{ $labels.name }}' on {{ $labels.instance }} has high read rate (> 100 IOPS)",
              "summary": "Container high read rate"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001125692,
            "lastEvaluation": "2021-12-21T15:41:46.858332569Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Container high write rate",
            "query": "sum by(instance, name) (irate(container_fs_writes_total{name!=\"\"}[5m])) > 100",
            "duration": 120,
            "labels": {
              "info": "Container '{{ $labels.name }}' on {{ $labels.instance }} has high write rate (> 100 IOPS)",
              "severity": "major",
              "tags": "docker"
            },
            "annotations": {
              "description": "Container '{{ $labels.name }}' on {{ $labels.instance }} has high write rate (> 100 IOPS)",
              "summary": "Container high write rate"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000987207,
            "lastEvaluation": "2021-12-21T15:41:46.859459702Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.005043045,
        "lastEvaluation": "2021-12-21T15:41:46.855409479Z"
      },
      {
        "name": "iperf3 alerts",
        "file": "/prometheus_config/alert_rules.yml",
        "rules": [
          {
            "state": "firing",
            "name": "iperf3 probe failed",
            "query": "iperf3_success == 0",
            "duration": 900,
            "labels": {
              "info": "iperf3 probe failed to measure network speed on '{{ $labels.instance }}'",
              "severity": "warning",
              "tags": "iperf3, network"
            },
            "annotations": {
              "description": "iperf3 probe failed to measure network speed on '{{ $labels.instance }}'",
              "summary": "iperf3 probe failed"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "iperf3 probe failed",
                  "info": "iperf3 probe failed to measure network speed on 'iperf3-server'",
                  "instance": "iperf3-server",
                  "job": "iperf3-probe",
                  "severity": "warning",
                  "tags": "iperf3, network"
                },
                "annotations": {
                  "description": "iperf3 probe failed to measure network speed on 'iperf3-server'",
                  "summary": "iperf3 probe failed"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:30:25.749586597Z",
                "value": "0e+00"
              },
              {
                "labels": {
                  "alertname": "iperf3 probe failed",
                  "info": "iperf3 probe failed to measure network speed on '172.30.14.94'",
                  "instance": "172.30.14.94",
                  "job": "iperf3-probe",
                  "severity": "warning",
                  "tags": "iperf3, network"
                },
                "annotations": {
                  "description": "iperf3 probe failed to measure network speed on '172.30.14.94'",
                  "summary": "iperf3 probe failed"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:31:25.749586597Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001013057,
            "lastEvaluation": "2021-12-21T15:41:40.751011075Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Network slow (transmit)",
            "query": "iperf3_sent_bytes{instance!=\"iperf3-server\"} / iperf3_sent_seconds{instance!=\"iperf3-server\"} < 5 * 1024 * 1024",
            "duration": 60,
            "labels": {
              "info": "Transmiting speed in network is below 5 MB/s on '{{ $labels.instance }}'",
              "severity": "warning",
              "tags": "iperf3, network"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}b/s",
              "description": "Transmit speed in network is below 5 MB/s on '{{ $labels.instance }}'",
              "summary": "Network slow (transmit)"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000518945,
            "lastEvaluation": "2021-12-21T15:41:40.752026335Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Network slow (receive)",
            "query": "iperf3_received_bytes{instance!=\"iperf3-server\"} / iperf3_received_seconds{instance!=\"iperf3-server\"} < 5 * 1024 * 1024",
            "duration": 60,
            "labels": {
              "info": "Receiving speed in network is below 5 MB/s on '{{ $labels.instance }}'",
              "severity": "warning",
              "tags": "iperf3, network"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}b/s",
              "description": "Receiving speed in network is below 5 MB/s on '{{ $labels.instance }}'",
              "summary": "Network slow (receive)"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000163625,
            "lastEvaluation": "2021-12-21T15:41:40.752546537Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.001708096,
        "lastEvaluation": "2021-12-21T15:41:40.75100398Z"
      },
      {
        "name": "Blackbox DNS probe alerts",
        "file": "/prometheus_config/blackbox_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "DNS Not Responding",
            "query": "probe_success{job=\"blackbox-dns\"} == 0",
            "duration": 600,
            "labels": {
              "alarmcode": "300070",
              "info": "DNS on {{$labels.instance}} has been down for more than 10 minutes.",
              "severity": "major",
              "tags": "dns"
            },
            "annotations": {
              "description": "DNS on {{$labels.instance}} has been down for more than 10 minutes.",
              "summary": "DNS on {{$labels.instance}} is down"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000513798,
            "lastEvaluation": "2021-12-21T15:41:39.605119332Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.00052949,
        "lastEvaluation": "2021-12-21T15:41:39.605111774Z"
      },
      {
        "name": "Blackbox HTTP probe alerts",
        "file": "/prometheus_config/blackbox_alert_rules.yml",
        "rules": [
          {
            "state": "firing",
            "name": "HTTP Not Responding",
            "query": "probe_success{job=\"blackbox-http\"} == 0",
            "duration": 120,
            "labels": {
              "alarmcode": "300070",
              "info": "HTTP on {{$labels.instance}} is down",
              "severity": "minor",
              "tags": "http, webserver"
            },
            "annotations": {
              "summary": "HTTP on {{$labels.instance}} has been down for more than 2 minutes."
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "300070",
                  "alertname": "HTTP Not Responding",
                  "info": "HTTP on http://fmslin3/mns/ is down",
                  "instance": "http://fmslin3/mns/",
                  "job": "blackbox-http",
                  "severity": "minor",
                  "tags": "http, webserver"
                },
                "annotations": {
                  "summary": "HTTP on http://fmslin3/mns/ has been down for more than 2 minutes."
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:42.123835141Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000809873,
            "lastEvaluation": "2021-12-21T15:41:42.124435632Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "HTTP Probe Failing",
            "query": "up{job=\"blackbox-http\"} == 0 or probe_success{job=\"blackbox-http\"} == 0",
            "duration": 600,
            "labels": {
              "alarmcode": "300070",
              "info": "HTTP probe on {{$labels.instance}} has been failing for more than 10 minutes.",
              "severity": "minor",
              "tags": "http"
            },
            "annotations": {
              "summary": "HTTP probe on {{$labels.instance}} has been failing for more than 10 minutes."
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "300070",
                  "alertname": "HTTP Probe Failing",
                  "info": "HTTP probe on http://fmslin3/mns/ has been failing for more than 10 minutes.",
                  "instance": "http://fmslin3/mns/",
                  "job": "blackbox-http",
                  "severity": "minor",
                  "tags": "http"
                },
                "annotations": {
                  "summary": "HTTP probe on http://fmslin3/mns/ has been failing for more than 10 minutes."
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:42.123835141Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000736927,
            "lastEvaluation": "2021-12-21T15:41:42.125247715Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Too long request duration",
            "query": "avg_over_time(probe_http_duration_seconds{job=\"blackbox-http\"}[1h]) > 3",
            "duration": 300,
            "labels": {
              "info": "The '{{$labels.phase}}' phase of HTTP requests on '{{$labels.instance}}' exceeds 3 seconds in the last hour",
              "severity": "minor",
              "tags": "ssl, webserver"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}s",
              "description": "The '{{$labels.phase}}' phase of HTTP requests on '{{$labels.instance}}' exceeds 3 seconds in the last hour",
              "summary": "Too long request duration on '{{$labels.instance}}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.003123106,
            "lastEvaluation": "2021-12-21T15:41:42.125986311Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "SSL Cert Expiring Soon",
            "query": "(probe_ssl_earliest_cert_expiry{job=\"blackbox-http\"} - time()) / 3600 / 24 < 30",
            "duration": 60,
            "labels": {
              "alarmcode": "4100030",
              "info": "SSL cert on '{{$labels.instance}}' will expire soon",
              "severity": "warning",
              "tags": "ssl, webserver"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }} days",
              "description": "SSL cert will expire in {{ humanize $value}} days",
              "summary": "SSL cert on '{{$labels.instance}}' will expire soon"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "4100030",
                  "alertname": "SSL Cert Expiring Soon",
                  "info": "SSL cert on 'https://gitlab.iskratel.si' will expire soon",
                  "instance": "https://gitlab.iskratel.si",
                  "job": "blackbox-http",
                  "severity": "warning",
                  "tags": "ssl, webserver"
                },
                "annotations": {
                  "currentValue": "-570.2 days",
                  "description": "SSL cert will expire in -570.2 days",
                  "summary": "SSL cert on 'https://gitlab.iskratel.si' will expire soon"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:29:12.123835141Z",
                "value": "-5.702035199421285e+02"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000679956,
            "lastEvaluation": "2021-12-21T15:41:42.129110611Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.005365295,
        "lastEvaluation": "2021-12-21T15:41:42.124428653Z"
      },
      {
        "name": "Blackbox ICMP probe alerts",
        "file": "/prometheus_config/blackbox_alert_rules.yml",
        "rules": [
          {
            "state": "firing",
            "name": "ICMP ping failed",
            "query": "probe_success{job=\"blackbox-icmp\"} == 0",
            "duration": 60,
            "labels": {
              "alarmcode": "2809020",
              "info": "No connection to {{$labels.instance}}",
              "priority": "medium",
              "severity": "critical",
              "tags": "icmp, ping failed"
            },
            "annotations": {
              "description": "{{$labels.instance}} does not reply to ICMP pings",
              "summary": "No connection to {{$labels.instance}}"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "2809020",
                  "alertname": "ICMP ping failed",
                  "info": "No connection to fmslin2",
                  "instance": "fmslin2",
                  "job": "blackbox-icmp",
                  "priority": "medium",
                  "severity": "critical",
                  "tags": "icmp, ping failed"
                },
                "annotations": {
                  "description": "fmslin2 does not reply to ICMP pings",
                  "summary": "No connection to fmslin2"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:15.08393676Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000852911,
            "lastEvaluation": "2021-12-21T15:41:45.085354823Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "ICMP Availability is too low",
            "query": "avg_over_time(probe_success{job=\"blackbox-icmp\"}[1h]) * 100 < 90",
            "duration": 900,
            "labels": {
              "info": "Instance availability fell below 90% in last hour on {{$labels.instance}}",
              "priority": "medium",
              "severity": "major",
              "tags": "icmp, ping failed, availability"
            },
            "annotations": {
              "currentValue": "{{ $value }}%",
              "description": "Instance availability fell below 90% in last hour",
              "summary": "Availability is too low on {{$labels.instance}}"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "ICMP Availability is too low",
                  "info": "Instance availability fell below 90% in last hour on fmslin2",
                  "instance": "fmslin2",
                  "job": "blackbox-icmp",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "icmp, ping failed, availability"
                },
                "annotations": {
                  "currentValue": "0%",
                  "description": "Instance availability fell below 90% in last hour",
                  "summary": "Availability is too low on fmslin2"
                },
                "state": "firing",
                "activeAt": "2021-11-29T14:34:15Z",
                "value": "0e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001141132,
            "lastEvaluation": "2021-12-21T15:41:45.086209484Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.002006168,
        "lastEvaluation": "2021-12-21T15:41:45.085347851Z"
      },
      {
        "name": "Blackbox SSH probe alerts",
        "file": "/prometheus_config/blackbox_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "SSH Not Responding",
            "query": "probe_success{job=\"blackbox-ssh\"} == 0",
            "duration": 600,
            "labels": {
              "alarmcode": "300070",
              "info": "SSH on {{$labels.instance}} has been down for more than 10 minutes.",
              "severity": "minor",
              "tags": "ssh"
            },
            "annotations": {
              "description": "SSH on {{$labels.instance}} has been down for more than 10 minutes.",
              "summary": "SSH on {{$labels.instance}} is down"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00044648,
            "lastEvaluation": "2021-12-21T15:41:40.323392285Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.00045924,
        "lastEvaluation": "2021-12-21T15:41:40.323383602Z"
      },
      {
        "name": "Docker alert rules",
        "file": "/prometheus_config/docker_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Manager is down",
            "query": "swarm_manager_nodes{state=\"down\"} > 0",
            "duration": 1,
            "labels": {
              "alarmcode": "5500120",
              "info": "Swarm has lost a manager",
              "priority": "medium",
              "severity": "major",
              "tags": "docker, swarm"
            },
            "annotations": {
              "description": "At least one manager is missing",
              "summary": "Swarm has lost a manager"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000541927,
            "lastEvaluation": "2021-12-21T15:41:44.089055415Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Swarm cluster quorum is lost",
            "query": "sum by(instance) (swarm_manager_nodes) / 2 < sum by(instance) (swarm_manager_nodes{state!=\"ready\"})",
            "duration": 60,
            "labels": {
              "alarmcode": "5500120",
              "info": "Too few Swarm managers are alive on '{{ $labels.instance }}'",
              "priority": "medium",
              "severity": "critical",
              "tags": "docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "Swarm cluster has failed as the quorum is lost",
              "summary": "Too few Swarm managers are alive on '{{ $labels.instance }}'",
              "todo": "Check status of nodes in Docker"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000656538,
            "lastEvaluation": "2021-12-21T15:41:44.089599923Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Pending tasks",
            "query": "sum by(instance, job) (avg_over_time(swarm_manager_tasks_total{state=~\"accepted|assigned|new|pending|preparing|ready|starting\"}[1m])) > 0",
            "duration": 300,
            "labels": {
              "info": "Some tasks are attempting to be started for more than 5 minutes",
              "priority": "low",
              "severity": "minor",
              "tags": "docker, swarm"
            },
            "annotations": {
              "description": "Some tasks are attempting to be started for more than 5 minutes which indicates that some containers do not want to start",
              "summary": "Too many pending tasks"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00066415,
            "lastEvaluation": "2021-12-21T15:41:44.090257657Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Swarm node is missing",
            "query": "sum by(instance) (swarm_manager_nodes{state=\"down\"}) > 0",
            "duration": 3600,
            "labels": {
              "alarmcode": "5500120",
              "info": "Swarm node is missing '{{ $labels.instance }}'",
              "priority": "low",
              "severity": "major",
              "tags": "docker, swarm"
            },
            "annotations": {
              "summary": "Swarm node is missing '{{ $labels.instance }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000276774,
            "lastEvaluation": "2021-12-21T15:41:44.090923064Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Swarm cluster quorum almost lost",
            "query": "sum by(instance) (swarm_manager_nodes) / 2 < sum by(instance) (swarm_manager_nodes{state!=\"ready\"})",
            "duration": 60,
            "labels": {
              "alarmcode": "5500120",
              "info": "Too few Swarm managers are alive on '{{ $labels.instance }}'",
              "priority": "medium",
              "severity": "critical",
              "tags": "docker, swarm"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "Swarm cluster will fail if quorum is lost",
              "summary": "Too few Swarm managers are alive on '{{ $labels.instance }}'",
              "todo": "Check status of nodes in Docker"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000562021,
            "lastEvaluation": "2021-12-21T15:41:44.091201496Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.002748085,
        "lastEvaluation": "2021-12-21T15:41:44.089018378Z"
      },
      {
        "name": "Grok exporter alerts",
        "file": "/prometheus_config/grok_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Too many login attempts",
            "query": "increase(log_systemd_logins_total[10m]) > 10",
            "duration": 60,
            "labels": {
              "info": "Too many login attempts for user {{ $labels.user }} detected on '{{ $labels.instance }}'",
              "severity": "major",
              "tags": "logfile, grok, security"
            },
            "annotations": {
              "currentValue": "{{ $value }}",
              "description": "Too many login attempts for user {{ $labels.user }} detected",
              "summary": "Too many login attempts  on '{{ $labels.instance }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000727121,
            "lastEvaluation": "2021-12-21T15:41:41.576971372Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Brute force attack detected",
            "query": "increase(log_unix_chkpwd_fails_total[5m]) > 10",
            "duration": 60,
            "labels": {
              "info": "Too many failed password authentications for user {{ $labels.user }}",
              "priority": "high",
              "severity": "critical",
              "tags": "logfile, grok, security"
            },
            "annotations": {
              "currentValue": "{{ $value }}",
              "description": "Too many failed password authentications for user {{ $labels.user }}",
              "summary": "Too many failed login attempts  on '{{ $labels.instance }}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000215294,
            "lastEvaluation": "2021-12-21T15:41:41.577701115Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.000979361,
        "lastEvaluation": "2021-12-21T15:41:41.576940894Z"
      },
      {
        "name": "CPU alerts",
        "file": "/prometheus_config/node_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Process intense CPU usage",
            "query": "rate(process_cpu_seconds_total[5m]) * 100 > 70",
            "duration": 60,
            "labels": {
              "alarmcode": "3900020",
              "info": "Process {{ $labels.job }} intense CPU usage on {{ $labels.instance }}",
              "severity": "warning",
              "tags": "server, cpu"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "Process {{ $labels.job }} is intensively using CPU on {{ $labels.instance }}. CPU usage is '{{ humanize $value }}%'",
              "summary": "High CPU usage on {{ $labels.instance }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001199257,
            "lastEvaluation": "2021-12-21T15:41:47.557650246Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "High CPU Usage",
            "query": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80",
            "duration": 60,
            "labels": {
              "alarmcode": "3900020",
              "info": "High CPU usage on {{ $labels.instance }} with duration {{ humanizeDuration $value }}",
              "severity": "major",
              "tags": "server, cpu",
              "url": "https://${GRAFANA_HOSTNAME}/grafana/d/7pMVh5Kmz/server-monitor"
            },
            "annotations": {
              "currentValue": "{{ $value }}%",
              "description": "{{ $labels.instance }} is using a lot of CPU. CPU usage is {{ $value }}%.",
              "summary": "High CPU usage on {{ $labels.instance }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001741672,
            "lastEvaluation": "2021-12-21T15:41:47.558851206Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "High Node CPU",
            "query": "instance:node_cpu_seconds_total:avg_rate5m > 80",
            "duration": 60,
            "labels": {
              "alarmcode": "3900020",
              "info": "High Node CPU for 1 min",
              "instance": "{{$labels.instance}}",
              "severity": "warning",
              "tags": "server, cpu"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "summary": "High Node CPU for 1 min"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000245803,
            "lastEvaluation": "2021-12-21T15:41:47.560594499Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "High Load",
            "query": "node_load5 > 2",
            "duration": 120,
            "labels": {
              "alarmcode": "6103830",
              "info": "High load on '{{ $labels.instance}}'",
              "severity": "minor",
              "tags": "server, cpu"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "Load average is {{ humanize $value}}.'",
              "summary": "High load on '{{ $labels.instance}}'"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "6103830",
                  "alertname": "High Load",
                  "info": "High load on 'worker-spa2.devops.iskratel.cloud:9100'",
                  "instance": "worker-spa2.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "minor",
                  "tags": "server, cpu"
                },
                "annotations": {
                  "currentValue": "2.09",
                  "description": "Load average is 2.09.'",
                  "summary": "High load on 'worker-spa2.devops.iskratel.cloud:9100'"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:57:47.556440753Z",
                "value": "2.09e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000976633,
            "lastEvaluation": "2021-12-21T15:41:47.560842662Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Server Overloaded",
            "query": "avg by(instance) (node_load1{job=~\"node-exporter.*\"}) / count by(instance) (node_cpu_seconds_total{mode=\"idle\"}) > 1",
            "duration": 300,
            "labels": {
              "alarmcode": "6103830",
              "info": "Load on '{{ $labels.instance}}' is higher than number of cores",
              "severity": "major",
              "tags": "server, cpu, load"
            },
            "annotations": {
              "currentValue": "{{ $value }}",
              "description": "There are more processes than all cores can handle",
              "summary": "Server Overloaded '{{ $labels.instance}}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00176294,
            "lastEvaluation": "2021-12-21T15:41:47.561821107Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.005952509,
        "lastEvaluation": "2021-12-21T15:41:47.557634697Z"
      },
      {
        "name": "Disk alerts",
        "file": "/prometheus_config/node_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Running Out Of Disk Space",
            "query": "(node_filesystem_size_bytes{mountpoint=\"/\"} - node_filesystem_free_bytes{mountpoint=\"/\"}) * 100 / node_filesystem_size_bytes{mountpoint=\"/\"} > 90",
            "duration": 1800,
            "labels": {
              "alarmcode": "3640060",
              "info": "Low disk space '{{ $labels.instance}}'",
              "severity": "major",
              "tags": "server, disk space"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "More than 90% of disk used. Disk usage {{ humanize $value}}%.",
              "summary": "Low disk space '{{ $labels.instance}}'"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.363510923,
            "lastEvaluation": "2021-12-21T15:41:35.568928573Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Out Of Disk Space",
            "query": "node_filesystem_free_bytes / node_filesystem_size_bytes * 100 < 10",
            "duration": 1800,
            "labels": {
              "alarmcode": "5500160",
              "info": "Disk is almost full (< 10% left)\n  LABELS: {{ $labels }}",
              "severity": "warning",
              "tags": "server, disk space"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}",
              "summary": "Out of disk space (instance {{ $labels.instance }})"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "5500160",
                  "alertname": "Out Of Disk Space",
                  "device": "/dev/mapper/VgMn-LvVar",
                  "fstype": "ext4",
                  "info": "Disk is almost full (< 10% left)\n  LABELS: map[device:/dev/mapper/VgMn-LvVar fstype:ext4 instance:dockermak.servis.mak:9100 job:node-exporter mountpoint:/var]",
                  "instance": "dockermak.servis.mak:9100",
                  "job": "node-exporter",
                  "mountpoint": "/var",
                  "severity": "warning",
                  "tags": "server, disk space"
                },
                "annotations": {
                  "currentValue": "5.585%",
                  "description": "Disk is almost full (< 10% left)\n  VALUE = 5.585350843884619\n  LABELS: map[device:/dev/mapper/VgMn-LvVar fstype:ext4 instance:dockermak.servis.mak:9100 job:node-exporter mountpoint:/var]",
                  "summary": "Out of disk space (instance dockermak.servis.mak:9100)"
                },
                "state": "firing",
                "activeAt": "2021-10-21T13:07:49Z",
                "value": "5.585350843884619e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.006703565,
            "lastEvaluation": "2021-12-21T15:41:35.932441582Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Unusually High Disk Read Rate",
            "query": "sum by(instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50",
            "duration": 600,
            "labels": {
              "info": "Disk is reading too much data (> 50 MB/s)",
              "severity": "minor",
              "tags": "server, disk space, iops"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}b/s",
              "description": "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}",
              "summary": "Unusual disk read rate on instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001036579,
            "lastEvaluation": "2021-12-21T15:41:35.939147649Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Unusually High Disk Write Rate",
            "query": "sum by(instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50",
            "duration": 600,
            "labels": {
              "info": "Disk is writing too much data (> 50 MB/s)",
              "severity": "minor",
              "tags": "server, disk space"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}b/s",
              "description": "Disk is probably writing too much data (> 50 MB/s)",
              "summary": "Unusual disk write rate (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000974501,
            "lastEvaluation": "2021-12-21T15:41:35.94018531Z",
            "type": "alerting"
          },
          {
            "state": "pending",
            "name": "High IOPs (read)",
            "query": "sum by(instance) (rate(node_disk_reads_completed_total[5m])) > 100",
            "duration": 600,
            "labels": {
              "info": "High reading IOPs (> 100)",
              "severity": "major",
              "tags": "server, disk space, iops"
            },
            "annotations": {
              "currentValue": "{{ $value }}",
              "summary": "High reading IOPs on {{ $labels.instance }}"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "High IOPs (read)",
                  "info": "High reading IOPs (> 100)",
                  "instance": "worker-spa2.devops.iskratel.cloud:9100",
                  "severity": "major",
                  "tags": "server, disk space, iops"
                },
                "annotations": {
                  "currentValue": "223.03157894736842",
                  "summary": "High reading IOPs on worker-spa2.devops.iskratel.cloud:9100"
                },
                "state": "pending",
                "activeAt": "2021-12-21T15:37:20.567234194Z",
                "value": "2.2303157894736842e+02"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001199118,
            "lastEvaluation": "2021-12-21T15:41:35.94116057Z",
            "type": "alerting"
          },
          {
            "state": "pending",
            "name": "High IOPs (write)",
            "query": "sum by(instance) (rate(node_disk_writes_completed_total[5m])) > 100",
            "duration": 600,
            "labels": {
              "info": "High writing IOPs (> 100)",
              "severity": "major",
              "tags": "server, disk space, iops"
            },
            "annotations": {
              "currentValue": "{{ $value }}",
              "summary": "High writing IOPs on {{ $labels.instance }}"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "High IOPs (write)",
                  "info": "High writing IOPs (> 100)",
                  "instance": "worker-spa2.devops.iskratel.cloud:9100",
                  "severity": "major",
                  "tags": "server, disk space, iops"
                },
                "annotations": {
                  "currentValue": "864.8561403508771",
                  "summary": "High writing IOPs on worker-spa2.devops.iskratel.cloud:9100"
                },
                "state": "pending",
                "activeAt": "2021-12-21T15:37:50.567234194Z",
                "value": "8.648561403508771e+02"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001319004,
            "lastEvaluation": "2021-12-21T15:41:35.942361155Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Unusual Disk Read Latency",
            "query": "rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) * 1000 > 100",
            "duration": 600,
            "labels": {
              "info": "Disk latency is growing (read operations > 100ms)\n  LABELS: {{ $labels }}",
              "severity": "minor",
              "tags": "server, disk space"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}s",
              "description": "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}",
              "summary": "Unusual disk read latency (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001967155,
            "lastEvaluation": "2021-12-21T15:41:35.943689883Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Unusual Disk Write Latency",
            "query": "rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) * 1000 > 100",
            "duration": 600,
            "labels": {
              "info": "Disk latency is growing (write operations > 100ms)\n  LABELS: {{ $labels }}",
              "severity": "minor",
              "tags": "server, disk space"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}s",
              "description": "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}",
              "summary": "Unusual disk write latency (instance {{ $labels.instance }})"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001987784,
            "lastEvaluation": "2021-12-21T15:41:35.945658073Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Swap used",
            "query": "node_memory_SwapCached_bytes > 0",
            "duration": 3600,
            "labels": {
              "info": "Swap is used on {{ $labels.instance }}",
              "severity": "informational",
              "tags": "server, swap"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}b",
              "description": "Using {{ humanize $value }}b swap",
              "summary": "Swap is used on {{ $labels.instance }}"
            },
            "alerts": [
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on orchestrator-iskratel.devops.iskratel.cloud:9100",
                  "instance": "orchestrator-iskratel.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "1.27Mb",
                  "description": "Using 1.27Mb swap",
                  "summary": "Swap is used on orchestrator-iskratel.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-21T06:17:35Z",
                "value": "1.26976e+06"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on dockermak.servis.mak:9100",
                  "instance": "dockermak.servis.mak:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "360.4kb",
                  "description": "Using 360.4kb swap",
                  "summary": "Swap is used on dockermak.servis.mak:9100"
                },
                "state": "firing",
                "activeAt": "2021-11-29T16:29:04Z",
                "value": "3.60448e+05"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on prom.devops.iskratel.cloud:9100",
                  "instance": "prom.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "11.56Mb",
                  "description": "Using 11.56Mb swap",
                  "summary": "Swap is used on prom.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:50.567234194Z",
                "value": "1.1563008e+07"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on 10.0.35.40:9100",
                  "instance": "10.0.35.40:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "11.56Mb",
                  "description": "Using 11.56Mb swap",
                  "summary": "Swap is used on 10.0.35.40:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:20.567234194Z",
                "value": "1.1563008e+07"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on docker-swarm.devops.iskratel.cloud:9100",
                  "instance": "docker-swarm.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "12.09Mb",
                  "description": "Using 12.09Mb swap",
                  "summary": "Swap is used on docker-swarm.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-13T14:38:35Z",
                "value": "1.2091392e+07"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on worker-spa2.devops.iskratel.cloud:9100",
                  "instance": "worker-spa2.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "82.63Mb",
                  "description": "Using 82.63Mb swap",
                  "summary": "Swap is used on worker-spa2.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-11-29T16:29:04Z",
                "value": "8.2632704e+07"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on worker-solid.devops.iskratel.cloud:9100",
                  "instance": "worker-solid.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "180.2kb",
                  "description": "Using 180.2kb swap",
                  "summary": "Swap is used on worker-solid.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-11-29T16:29:04Z",
                "value": "1.80224e+05"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on openbaton.devops.iskratel.cloud:9100",
                  "instance": "openbaton.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "8.839Mb",
                  "description": "Using 8.839Mb swap",
                  "summary": "Swap is used on openbaton.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-21T07:21:20Z",
                "value": "8.839168e+06"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on worker2.devops.iskratel.cloud:9100",
                  "instance": "worker2.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "10.28Mb",
                  "description": "Using 10.28Mb swap",
                  "summary": "Swap is used on worker2.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:50.567234194Z",
                "value": "1.0276864e+07"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on mcrk-docker-1:9100",
                  "instance": "mcrk-docker-1:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "114.7kb",
                  "description": "Using 114.7kb swap",
                  "summary": "Swap is used on mcrk-docker-1:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-13T00:45:35Z",
                "value": "1.14688e+05"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on jenkins.devops.iskratel.cloud:9100",
                  "instance": "jenkins.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "307.2kb",
                  "description": "Using 307.2kb swap",
                  "summary": "Swap is used on jenkins.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-13T14:38:35Z",
                "value": "3.072e+05"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on ipa-master.devops.iskratel.cloud:9100",
                  "instance": "ipa-master.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "806.9kb",
                  "description": "Using 806.9kb swap",
                  "summary": "Swap is used on ipa-master.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-11-29T16:29:04Z",
                "value": "8.06912e+05"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on ipa-replica1.devops.iskratel.cloud:9100",
                  "instance": "ipa-replica1.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "245.8kb",
                  "description": "Using 245.8kb swap",
                  "summary": "Swap is used on ipa-replica1.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-11-29T16:29:04Z",
                "value": "2.4576e+05"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on worker-spa.devops.iskratel.cloud:9100",
                  "instance": "worker-spa.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "68.19Mb",
                  "description": "Using 68.19Mb swap",
                  "summary": "Swap is used on worker-spa.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:50.567234194Z",
                "value": "6.8190208e+07"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on worker1.devops.iskratel.cloud:9100",
                  "instance": "worker1.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "618.5kb",
                  "description": "Using 618.5kb swap",
                  "summary": "Swap is used on worker1.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-21T06:54:20Z",
                "value": "6.18496e+05"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on worker3.devops.iskratel.cloud:9100",
                  "instance": "worker3.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "3.142Mb",
                  "description": "Using 3.142Mb swap",
                  "summary": "Swap is used on worker3.devops.iskratel.cloud:9100"
                },
                "state": "firing",
                "activeAt": "2021-11-29T16:29:04Z",
                "value": "3.141632e+06"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on 172.30.14.94:9100",
                  "instance": "172.30.14.94:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "12.44Mb",
                  "description": "Using 12.44Mb swap",
                  "summary": "Swap is used on 172.30.14.94:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-13T14:38:35Z",
                "value": "1.2439552e+07"
              },
              {
                "labels": {
                  "alertname": "Swap used",
                  "info": "Swap is used on pmon.compact.si:9100",
                  "instance": "pmon.compact.si:9100",
                  "job": "node-exporter",
                  "severity": "informational",
                  "tags": "server, swap"
                },
                "annotations": {
                  "currentValue": "55.68Mb",
                  "description": "Using 55.68Mb swap",
                  "summary": "Swap is used on pmon.compact.si:9100"
                },
                "state": "firing",
                "activeAt": "2021-12-14T23:05:50Z",
                "value": "5.5676928e+07"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.004806969,
            "lastEvaluation": "2021-12-21T15:41:35.947646889Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Swap utilization",
            "query": "(node_memory_SwapTotal_bytes{instance=~\"$node\"} - node_memory_SwapFree_bytes{instance=~\"$node\"}) / node_memory_SwapTotal_bytes{instance=~\"$node\"} * 100 > 20",
            "duration": 3600,
            "labels": {
              "info": "Swap utilization on {{ $labels.instance }}",
              "severity": "warning",
              "tags": "server, swap"
            },
            "annotations": {
              "currentValue": "{{ $value }}%",
              "description": "Using {{ $value }}% swap",
              "summary": "Swap utilization on {{ $labels.instance }}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000298271,
            "lastEvaluation": "2021-12-21T15:41:35.952456784Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Directory size too big",
            "query": "test_directory_size / 1024 / 1024 > 3",
            "duration": 600,
            "labels": {
              "info": "Directory {{$labels.directory}} has grown over 3 GB limit",
              "nodename": "{{$labels.hostname}}",
              "priority": "high",
              "severity": "minor",
              "tags": "server, disk space"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }} GB",
              "description": "{{$labels.directory}} size: {{ humanize $value}} GB.",
              "summary": "Directory {{$labels.directory}} is growing over limit"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000157835,
            "lastEvaluation": "2021-12-21T15:41:35.952755922Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Disk Will Fill In 4 Hours",
            "query": "predict_linear(node_filesystem_free_bytes{mountpoint=\"/\"}[1h], 4 * 3600) < 0",
            "duration": 300,
            "labels": {
              "info": "Disk on {{$labels.instance}} will fill in approximately 4 hours.",
              "severity": "major",
              "tags": "server, disk space, prediction"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "summary": "Disk on {{$labels.instance}} will fill in approximately 4 hours."
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.002394601,
            "lastEvaluation": "2021-12-21T15:41:35.952914946Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.386391898,
        "lastEvaluation": "2021-12-21T15:41:35.568921647Z"
      },
      {
        "name": "Memory alerts",
        "file": "/prometheus_config/node_alert_rules.yml",
        "rules": [
          {
            "state": "firing",
            "name": "High Memory Usage",
            "query": "((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * 100 > 80",
            "duration": 600,
            "labels": {
              "alarmcode": "3900030",
              "info": "High memory usage on {{ $labels.instance}}",
              "severity": "minor",
              "tags": "server, memory",
              "url": "https://${GRAFANA_HOSTNAME}/grafana/d/7pMVh5Kmz/server-monitor"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}%",
              "description": "Memory usage is over {{ humanize $value}}%.",
              "summary": "High memory usage on '{{ $labels.instance}}'"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "3900030",
                  "alertname": "High Memory Usage",
                  "info": "High memory usage on openbaton.devops.iskratel.cloud:9100",
                  "instance": "openbaton.devops.iskratel.cloud:9100",
                  "job": "node-exporter",
                  "severity": "minor",
                  "tags": "server, memory",
                  "url": "https://${GRAFANA_HOSTNAME}/grafana/d/7pMVh5Kmz/server-monitor"
                },
                "annotations": {
                  "currentValue": "80.86%",
                  "description": "Memory usage is over 80.86%.",
                  "summary": "High memory usage on 'openbaton.devops.iskratel.cloud:9100'"
                },
                "state": "firing",
                "activeAt": "2021-12-21T07:55:06Z",
                "value": "8.086032804701597e+01"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001386806,
            "lastEvaluation": "2021-12-21T15:41:36.329733799Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.001397196,
        "lastEvaluation": "2021-12-21T15:41:36.329726919Z"
      },
      {
        "name": "Systemd alerts",
        "file": "/prometheus_config/node_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Service Down",
            "query": "node_systemd_unit_state{state=\"active\"} != 1",
            "duration": 300,
            "labels": {
              "alarmcode": "2800000",
              "info": "{{$labels.name}} not active",
              "priority": "medium",
              "severity": "major",
              "tags": "systemd, service"
            },
            "annotations": {
              "summary": "Service {{$labels.name}} not active"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000315408,
            "lastEvaluation": "2021-12-21T15:41:39.182930332Z",
            "type": "alerting"
          },
          {
            "state": "firing",
            "name": "Systemd services not monitored",
            "query": "absent(node_systemd_unit_state{job=~\"node-exporter.*\"})",
            "duration": 600,
            "labels": {
              "alarmcode": "100170",
              "info": "Systemd services are not monitored!",
              "priority": "low",
              "severity": "minor",
              "tags": "systemd"
            },
            "annotations": {
              "summary": "Systemd services are not monitored!"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "100170",
                  "alertname": "Systemd services not monitored",
                  "info": "Systemd services are not monitored!",
                  "priority": "low",
                  "severity": "minor",
                  "tags": "systemd"
                },
                "annotations": {
                  "summary": "Systemd services are not monitored!"
                },
                "state": "firing",
                "activeAt": "2021-10-21T13:12:22Z",
                "value": "1e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.000532428,
            "lastEvaluation": "2021-12-21T15:41:39.183247032Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.000859348,
        "lastEvaluation": "2021-12-21T15:41:39.182923466Z"
      },
      {
        "name": "Prometheus alerts",
        "file": "/prometheus_config/prometheus_alert_rules.yml",
        "rules": [
          {
            "state": "inactive",
            "name": "Prometheus Config Reload Failed",
            "query": "prometheus_config_last_reload_successful == 0",
            "duration": 300,
            "labels": {
              "info": "Reloading Prometheus configuration has failed on {{$labels.instance}}",
              "priority": "medium",
              "severity": "warning",
              "tags": "application, prometheus"
            },
            "annotations": {
              "summary": "Reloading Prometheus configuration has failed on {{$labels.instance}}"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000488791,
            "lastEvaluation": "2021-12-21T15:41:32.925704123Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "No connection to Alertmanager",
            "query": "prometheus_notifications_alertmanagers_discovered < 1",
            "duration": 300,
            "labels": {
              "info": "Prometheus on {{$labels.instance}} is not connected to any alertmanager.",
              "priority": "medium",
              "severity": "warning",
              "tags": "application, prometheus"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "Prometheus on {{$labels.instance}} is not connected to any alertmanager.",
              "summary": "No connection to Alertmanager"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00015229,
            "lastEvaluation": "2021-12-21T15:41:32.926194615Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "All metrics gone",
            "query": "absent(up{job=\"node-exporter\"})",
            "duration": 10,
            "labels": {
              "info": "No metric for job '{{$labels.job}}' received at all",
              "instance": "prometheus",
              "nodename": "prometheus-local",
              "severity": "warning",
              "tags": "application, prometheus"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "OMG...",
              "summary": "No metric for job '{{$labels.job}}' received at all"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000477849,
            "lastEvaluation": "2021-12-21T15:41:32.926347697Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Too many alerts sent",
            "query": "sum by(alertname) (sort_desc(sum_over_time(ALERTS{alertstate=\"firing\"}[1h]))) / 60 / 4 > 100",
            "duration": 300,
            "labels": {
              "info": "You are alerting too much",
              "instance": "prometheus",
              "nodename": "prometheus-local",
              "priority": "low",
              "severity": "warning",
              "tags": "application, prometheus"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}/h",
              "description": "More than 100 alerts '{{$labels.alertname}}' sent in last hour.",
              "summary": "You are alerting too much"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.006268361,
            "lastEvaluation": "2021-12-21T15:41:32.926826577Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Prometheus HTTP high response time",
            "query": "rate(prometheus_http_request_duration_seconds_sum[1m]) / rate(prometheus_http_request_duration_seconds_count[1m]) > 0.1",
            "duration": 600,
            "labels": {
              "info": "Handler '{{$labels.handler}}' average response time exceeds 100ms.",
              "severity": "minor",
              "tags": "prometheus, http"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}s",
              "description": "Handler '{{$labels.handler}}' average response time exceeds 100ms.",
              "summary": "Prometheus HTTP high response time"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.00067012,
            "lastEvaluation": "2021-12-21T15:41:32.933097018Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Prometheus getting sluggish",
            "query": "sum by(handler) (rate(prometheus_http_request_duration_seconds_bucket{le=\"0.1\"}[5m])) / sum by(handler) (rate(prometheus_http_request_duration_seconds_count[5m])) < 0.8",
            "duration": 300,
            "labels": {
              "info": "Less than 80% of requests on endpoint '{{$labels.handler}}' served below 100ms",
              "instance": "prometheus-local",
              "priority": "medium",
              "severity": "minor",
              "tags": "prometheus, http, sla"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "Less than 80% of requests on '{{$labels.handler}}' endpoint served below 100ms",
              "summary": "Prometheus getting sluggish"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.000615514,
            "lastEvaluation": "2021-12-21T15:41:32.933768353Z",
            "type": "alerting"
          },
          {
            "state": "inactive",
            "name": "Prometheus responding slowly",
            "query": "histogram_quantile(0.5, sum by(le) (rate(prometheus_http_request_duration_seconds_bucket[5m]))) > 0.2",
            "duration": 300,
            "labels": {
              "info": "50% of requests took longer than 200ms.",
              "instance": "prometheus-local",
              "severity": "minor",
              "tags": "prometheus, http, sla"
            },
            "annotations": {
              "currentValue": "{{ humanize $value }}",
              "description": "50% of requests took longer than 200ms.",
              "summary": "Prometheus responding slowly"
            },
            "alerts": [],
            "health": "ok",
            "evaluationTime": 0.001479732,
            "lastEvaluation": "2021-12-21T15:41:32.934384664Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.010171195,
        "lastEvaluation": "2021-12-21T15:41:32.925696562Z"
      },
      {
        "name": "Container recording rules",
        "file": "/prometheus_config/recording_rules.yml",
        "rules": [
          {
            "name": "my_container_replicas_by_service",
            "query": "count by(container_label_com_docker_swarm_service_name) (container_last_seen * on(container_label_com_docker_swarm_node_id) group_left(node_name) min without(instance) (node_meta))",
            "labels": {
              "metric_type": "aggregation",
              "node_name": "{{ $labels.node_name }}"
            },
            "health": "ok",
            "evaluationTime": 0.002891399,
            "lastEvaluation": "2021-12-21T15:41:47.364790919Z",
            "type": "recording"
          }
        ],
        "interval": 10,
        "evaluationTime": 0.002909523,
        "lastEvaluation": "2021-12-21T15:41:47.364779418Z"
      },
      {
        "name": "SNMP exporter alerts",
        "file": "/prometheus_config/snmp_alert_rules.yml",
        "rules": [
          {
            "state": "firing",
            "name": "Interface down",
            "query": "avg by(ifIndex, ifDescr, instance, job) (ifOperStatus * on(ifIndex) group_left(ifDescr) min without(instance) (ifDescr{ifIndex=~\".*\"})) == 2",
            "duration": 60,
            "labels": {
              "alarmcode": "6600070",
              "info": "Interface {{$labels.ifDescr}}",
              "priority": "medium",
              "severity": "major",
              "tags": "snmp, interface"
            },
            "annotations": {
              "description": "ifIndex={{$labels.ifIndex}}, ifDescr={{$labels.ifDescr}}",
              "summary": "Interface {{$labels.ifDescr}} on {{$labels.instance}} is down"
            },
            "alerts": [
              {
                "labels": {
                  "alarmcode": "6600070",
                  "alertname": "Interface down",
                  "ifDescr": "VLAN ID 4093",
                  "ifIndex": "5093",
                  "info": "Interface VLAN ID 4093",
                  "instance": "192.168.140.4",
                  "job": "snmp-exporter",
                  "priority": "medium",
                  "severity": "major",
                  "tags": "snmp, interface"
                },
                "annotations": {
                  "description": "ifIndex=5093, ifDescr=VLAN ID 4093",
                  "summary": "Interface VLAN ID 4093 on 192.168.140.4 is down"
                },
                "state": "firing",
                "activeAt": "2021-12-21T14:28:54.339295778Z",
                "value": "2e+00"
              }
            ],
            "health": "ok",
            "evaluationTime": 0.001222676,
            "lastEvaluation": "2021-12-21T15:41:39.34010638Z",
            "type": "alerting"
          }
        ],
        "interval": 15,
        "evaluationTime": 0.001232626,
        "lastEvaluation": "2021-12-21T15:41:39.340100416Z"
      }
    ]
  }
}